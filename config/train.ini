[global]
# OPTIONS THAT APPLY TO ALL MODELS
# NB: UNLESS SPECIFICALLY STATED, VALUES CHANGED HERE WILL ONLY TAKE EFFECT WHEN CREATING A NEW MODEL.

# How much of the extracted image to train on. A lower coverage will limit the model's scope to a zoomed-in central area while higher amounts can include the entire face. A trade-off exists between lower amounts given more detail versus higher amounts avoiding noticeable swap transitions. Sensible values to use are:
# 	62.5%% spans from eyebrow to eyebrow.
# 	75.0%% spans from temple to temple.
# 	87.5%% spans from ear to ear.
# 	100.0%% is a mugshot.
# Select a decimal number between 62.5 and 100.0
# [Default: 68.75]
coverage = 68.75

# The mask to be used for training:
# 	 none: Doesn't use any mask.
# 	 components: An improved face hull mask using a facehull of 8 facial parts
# 	 dfl_full: An improved face hull mask using a facehull of 3 facial parts
# 	 extended: Based on components mask. Extends the eyebrow points to further up the forehead. May perform badly on difficult angles.
# 	 facehull: Face cutout based on landmarks
# Choose from: ['components', 'dfl_full', 'extended', 'facehull', 'none']
# [Default: none]
mask_type = none

# Apply gaussian blur to the mask input. This has the effect of smoothing the edges of the mask, which can help with poorly calculated masks, and give less of a hard edge to the predicted mask.
# Choose from: True, False
# [Default: False]
mask_blur = False

# Use ICNR to tile the default initializer in a repeating pattern. This strategy is designed for pairing with sub-pixel / pixel shuffler to reduce the 'checkerboard effect' in image reconstruction. 
# 	 https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf
# Choose from: True, False
# [Default: False]
icnr_init = False

# Use Convolution Aware Initialization for convolutional layers. This can help eradicate the vanishing and exploding gradient problem as well as lead to higher accuracy, lower loss and faster convergence.
# NB:
# 	 This can use more VRAM when creating a new model so you may want to lower the batch size for the first run. The batch size can be raised again when reloading the model. 
# 	 Multi-GPU is not supported for this option, so you should start the model on a single GPU. Once training has started, you can stop training, enable multi-GPU and resume.
# 	 Building the model will likely take several minutes as the calculations for this initialization technique are expensive. This will only impact starting a new model.
# Choose from: True, False
# [Default: False]
conv_aware_init = False

# Use subpixel upscaling rather than pixel shuffler. These techniques are both designed to produce better resolving upscaling than other methods. Each perform the same operations, but using different TF opts.
# 	 https://arxiv.org/pdf/1609.05158.pdf
# Choose from: True, False
# [Default: False]
subpixel_upscaling = False

# Use reflection padding rather than zero padding with convolutions. Each convolution must pad the image boundaries to maintain the proper sizing. More complex padding schemes can reduce artifacts at the border of the image.
# 	 http://www-cs.engr.ccny.cuny.edu/~wolberg/cs470/hw/hw2_pad.txt
# Choose from: True, False
# [Default: False]
reflect_padding = False

# Image loss function is weighted by mask presence. For areas of the image without the facial mask, reconstuction errors will be ignored while the masked face area is prioritized. May increase overall quality by focusing attention on the core face area.
# Choose from: True, False
# [Default: True]
penalized_mask_loss = True

# 
# 	 MAE - Mean absolute error will guide reconstructions of each pixel towards its median value in the training dataset. Robust to outliers but as a median, it can potentially ignore some infrequent image types in the dataset.
# 	 MSE - Mean squared error will guide reconstructions of each pixel towards its average value in the training dataset. As an avg, it will be suspectible to outliers and typically produces slightly blurrier results.
# 	 LogCosh - log(cosh(x)) acts similiar to MSE for small errors and to MAE for large errors. Like MSE, it is very stable and prevents overshoots when errors are near zero. Like MAE, it is robust to outliers.
# 	 Smooth_L1 --- Modification of the MAE loss to correct two of its disadvantages. This loss has improved stability and guidance for small errors.
# 	 L_inf_norm --- The L_inf norm will reduce the largest individual pixel error in an image. As each largest error is minimized sequentially, the overall error is improved. This loss will be extremely focused on outliers.
# 	 SSIM - Structural Similarity Index Metric is a perception-based loss that considers changes in texture, luminance, contrast, and local spatial statistics of an image. Potentially delivers more realistic looking images.
# 	 GMSD - Gradient Magnitude Similarity Deviation seeks to match the global standard deviation of the pixel to pixel differences between two images. Similiar in approach to SSIM.
# 	 Pixel_Gradient_Difference - Instead of minimizing the difference between the absolute value of each pixel in two reference images, compute the pixel to pixel spatial difference in each image and then minimize that difference between two images. Allows for large color shifts,but maintains the structure of the image.
# 
# Choose from: ['mae', 'mse', 'logcosh', 'smooth_l1', 'l_inf_norm', 'ssim', 'gmsd', 'pixel_gradient_diff']
# [Default: mae]
loss_function = mae

# Learning rate - how fast your network will learn (how large are the modifications to the model weights after one batch of training). Values that are too large might result in model crashes and the inability of the model to find the best solution. Values that are too small might be unable to escape from dead-ends and find the best global minimum.
# This option can be updated for existing models.
# Select a decimal number between 1e-06 and 0.0001
# [Default: 5e-05]
learning_rate = 5e-05

[model.dfl_h128]
# DFL H128 MODEL (ADAPTED FROM HTTPS://GITHUB.COM/IPEROV/DEEPFACELAB)
# NB: UNLESS SPECIFICALLY STATED, VALUES CHANGED HERE WILL ONLY TAKE EFFECT WHEN CREATING A NEW MODEL.

# Lower memory mode. Set to 'True' if having issues with VRAM useage.
# NB: Models with a changed lowmem mode are not compatible with each other.
# Choose from: True, False
# [Default: False]
lowmem = False

[model.dfl_sae]
# DFL SAE MODEL (ADAPTED FROM HTTPS://GITHUB.COM/IPEROV/DEEPFACELAB)
# NB: UNLESS SPECIFICALLY STATED, VALUES CHANGED HERE WILL ONLY TAKE EFFECT WHEN CREATING A NEW MODEL.

# Resolution (in pixels) of the input image to train on.
# BE AWARE Larger resolution will dramatically increase VRAM requirements.
# 
# Must be divisible by 16.
# Select an integer between 64 and 256
# [Default: 128]
input_size = 128

# Controls gradient clipping of the optimizer. Can prevent model corruption at the expense of VRAM.
# This option can be updated for existing models.
# Choose from: True, False
# [Default: True]
clipnorm = True

# Model architecture:
# 	'df': Keeps the faces more natural.
# 	'liae': Can help fix overly different face shapes.
# Choose from: ['df', 'liae']
# [Default: df]
architecture = df

# Face information is stored in AutoEncoder dimensions. If there are not enough dimensions then certain facial features may not be recognized.
# Higher number of dimensions are better, but require more VRAM.
# Set to 0 to use the architecture defaults (256 for liae, 512 for df).
# Select an integer between 0 and 1024
# [Default: 0]
autoencoder_dims = 0

# Encoder dimensions per channel. Higher number of encoder dimensions will help the model to recognize more facial features, but will require more VRAM.
# Select an integer between 21 and 85
# [Default: 42]
encoder_dims = 42

# Decoder dimensions per channel. Higher number of decoder dimensions will help the model to improve details, but will require more VRAM.
# Select an integer between 10 and 85
# [Default: 21]
decoder_dims = 21

# Multiscale decoder can help to obtain better details.
# Choose from: True, False
# [Default: False]
multiscale_decoder = False

[model.original]
# ORIGINAL FACESWAP MODEL.
# NB: UNLESS SPECIFICALLY STATED, VALUES CHANGED HERE WILL ONLY TAKE EFFECT WHEN CREATING A NEW MODEL.

# Lower memory mode. Set to 'True' if having issues with VRAM useage.
# NB: Models with a changed lowmem mode are not compatible with each other.
# Choose from: True, False
# [Default: False]
lowmem = False

[model.realface]
# AN EXTRA DETAILED VARIANT OF ORIGINAL MODEL.
# INCORPORATES IDEAS FROM BRYANLYON AND INSPIRATION FROM THE VILLAIN MODEL.
# REQUIRES ABOUT 6GB-8GB OF VRAM (BATCHSIZE 8-16).
# 
# NB: UNLESS SPECIFICALLY STATED, VALUES CHANGED HERE WILL ONLY TAKE EFFECT WHEN CREATING A NEW MODEL.

# Resolution (in pixels) of the input image to train on.
# BE AWARE Larger resolution will dramatically increase VRAM requirements.
# Higher resolutions may increase prediction accuracy, but does not effect the resulting output size.
# Must be between 64 and 128 and be divisible by 16.
# Select an integer between 64 and 128
# [Default: 64]
input_size = 64

# Output image resolution (in pixels).
# Be aware that larger resolution will increase VRAM requirements.
# NB: Must be between 64 and 256 and be divisible by 16.
# Select an integer between 64 and 256
# [Default: 128]
output_size = 128

# Number of nodes for decoder. Might affect your model's ability to learn in general.
# Note that: Lower values will affect the ability to predict details.
# Select an integer between 768 and 2048
# [Default: 1536]
dense_nodes = 1536

# Encoder Convolution Layer Complexity. sensible ranges: 128 to 150.
# Select an integer between 96 and 160
# [Default: 128]
complexity_encoder = 128

# Decoder Complexity.
# Select an integer between 512 and 544
# [Default: 512]
complexity_decoder = 512

[model.unbalanced]
# AN UNBALANCED MODEL WITH ADJUSTABLE INPUT SIZE OPTIONS.
# THIS IS AN UNBALANCED MODEL SO B>A SWAPS MAY NOT WORK WELL
# 
# NB: UNLESS SPECIFICALLY STATED, VALUES CHANGED HERE WILL ONLY TAKE EFFECT WHEN CREATING A NEW MODEL.

# Resolution (in pixels) of the image to train on.
# BE AWARE Larger resolution will dramatically increaseVRAM requirements.
# Make sure your resolution is divisible by 64 (e.g. 64, 128, 256 etc.).
# NB: Your faceset must be at least 1.6x larger than your required input size.
# (e.g. 160 is the maximum input size for a 256x256 faceset).
# Select an integer between 64 and 512
# [Default: 128]
input_size = 128

# Lower memory mode. Set to 'True' if having issues with VRAM useage.
# NB: Models with a changed lowmem mode are not compatible with each other.
# NB: lowmem will override cutom nodes and complexity settings.
# Choose from: True, False
# [Default: False]
lowmem = False

# Controls gradient clipping of the optimizer. Can prevent model corruption at the expense of VRAM.
# Choose from: True, False
# [Default: True]
clipnorm = True

# Number of nodes for decoder. Don't change this unless you know what you are doing!
# Select an integer between 512 and 4096
# [Default: 1024]
nodes = 1024

# Encoder Convolution Layer Complexity. sensible ranges: 128 to 160.
# Select an integer between 64 and 1024
# [Default: 128]
complexity_encoder = 128

# Decoder A Complexity.
# Select an integer between 64 and 1024
# [Default: 384]
complexity_decoder_a = 384

# Decoder B Complexity.
# Select an integer between 64 and 1024
# [Default: 512]
complexity_decoder_b = 512

[model.villain]
# A HIGHER RESOLUTION VERSION OF THE ORIGINAL MODEL BY VILLAINGUY.
# EXTREMELY VRAM HEAVY. FULL MODEL REQUIRES 9GB+ FOR BATCHSIZE 16
# 
# NB: UNLESS SPECIFICALLY STATED, VALUES CHANGED HERE WILL ONLY TAKE EFFECT WHEN CREATING A NEW MODEL.

# Lower memory mode. Set to 'True' if having issues with VRAM useage.
# NB: Models with a changed lowmem mode are not compatible with each other.
# Choose from: True, False
# [Default: False]
lowmem = False

[trainer.original]
# ORIGINAL TRAINER OPTIONS.
# WARNING: THE DEFAULTS FOR AUGMENTATION WILL BE FINE FOR 99.9% OF USE CASES. ONLY CHANGE THEM IF YOU ABSOLUTELY KNOW WHAT YOU ARE DOING!

# Number of sample faces to display for each side in the preview when training.
# Select an integer between 2 and 16
# [Default: 14]
preview_images = 14

# Percentage amount to randomly zoom each training image in and out.
# Select an integer between 0 and 25
# [Default: 5]
zoom_amount = 5

# Percentage amount to randomly rotate each training image.
# Select an integer between 0 and 25
# [Default: 10]
rotation_range = 10

# Percentage amount to randomly shift each training image horizontally and vertically.
# Select an integer between 0 and 25
# [Default: 5]
shift_range = 5

# Percentage chance to randomly flip each training image horizontally.
# NB: This is ignored if the 'no-flip' option is enabled
# Select an integer between 0 and 75
# [Default: 50]
flip_chance = 50

# Percentage amount to randomly alter the lightness of each training image.
# NB: This is ignored if the 'no-augment-color' option is enabled
# Select an integer between 0 and 75
# [Default: 30]
color_lightness = 30

# Percentage amount to randomly alter the 'a' and 'b' colors of the L*a*b* color space of each training image.
# NB: This is ignored if the 'no-augment-color' option is enabled
# Select an integer between 0 and 50
# [Default: 8]
color_ab = 8

# Percentage chance to perform Contrast Limited Adaptive Histogram Equalization on each training image.
# NB: This is ignored if the 'no-augment-color' option is enabled
# This option can be updated for existing models.
# Select an integer between 0 and 75
# [Default: 50]
color_clahe_chance = 50

# The grid size dictates how much Contrast Limited Adaptive Histogram Equalization is performed on any training image selected for clahe. Contrast will be applied randomly with a gridsize of 0 up to the maximum. This value is a multiplier calculated from the training image size.
# NB: This is ignored if the 'no-augment-color' option is enabled
# Select an integer between 1 and 8
# [Default: 4]
color_clahe_max_size = 4

